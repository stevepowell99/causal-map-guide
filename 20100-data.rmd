# Core tables in Causal Map{#core-tables}

![image-20210913215748583](_assets/image-20210913215748583.png)

Each table has the following types of field:

- ID field
- Standard fields e.g. statement_memo. Users enter data into these fields.
- Custom fields which users can add, as long as their names are not reserved field names
- System fields like *frequency* which are added by the system. These are ignored when re-uploading an exported file.

## General principles

- don't have empty tabs in your Excel file, just delete them
- don't have empty fields aka columns, just delete them
- all 5 standard tables can also have arbitrary additional columns/fields which users can use e.g. to filter, analyse their data.
- there is no appending, just download the old file, edit / add / delete your data, upload again
    - this means that if you delete some rows and upload, the corresponding rows are deleted

Your Excel file can have different tabs. The app will read any tabs with any of these (lowercase) names: factors, links, statements, sources, questions, settings and will ignore other tabs.

Each tab must have a header row with the names of the fields.

These are the standard fields. You can add other custom fields too if you wish, and they will be available to the app.

## The main five tables

### Factors

### Links

- from
- to ((the from and to fields should contain ids which match the factor_id in the factors table - or if you know nothing else about the factors, just don't include a factors tab in the file.))
- statement_id
- link_memo


### Statements

- statement_id
- source_id
- question_id
- statement_memo

### Uploading/Importing from Word or other text files.

The only data you need to provide is a table of statements as the only tab in an xlsx file. Your file must have a header row with the names of the fields.

### The easiest way

Each paragraph in your Word document will become one statement, so make sure your Word document is already structured so this will work for you. Delete any double paragraph breaks in Word (i.e. delete any empty paragraphs). If you want some short paragraphs to appear together as one longer statement, delete the paragraph breaks between them.

Then

- Copy all the text from your document
- Create a fresh spreadsheet file in Excel (or LibreOffice Calc or similar) and select cell A2
- Paste your text.
- Type the word “text” as column header in cell A1.
- Check the statements are the way you want them. You can ignore any formatting, pictures etc which might also have been pasted in because these will disappear in the next step.

The statements are the texts which appear one by one by one in the Code & View tab, for you to read and code. Each statement is one row in your file.

If you want to import interviews which are several pages each, you should break each interview into several statements. Ideally a statement consists of between one and five paragraphs – enough to fit on the left-hand side of the Code & View tab when it is displayed there. Usually your text breaks up naturally into sections, for example the interviews might be responses to a number of questions, so your statements might consist of one or a few paragraphs for each question.

We strongly recommend that you don’t try to import statements which are longer than 2500 characters or 500 words. It is harder to code very long statements.

Your statements file must have at least one column with the text of the statements and the header for this column must be “text.” Other columns will be treated as additional data.

The only required field in your data table is the text of each statement, like this.

**text**

---

Lorem ipsum …

---

Ipsum lorem …

---

Lorem sunt …

---

Asdf …

---

…

---

However you nearly always have additional information, first and foremost about the source, for example about the respondent who provided the information, or about the context of the information, e.g. if this statement was a response to a question, what is the question number, or the text of the question, or the name of the interviewer. You can think of additional data as just any additional fields in the statements table. If you provide all of these fields when you import your data, that’s fine, there is nothing more to do. For example, you can import the table below instead of the table above. At the moment the app understands only text fields and numerical fields. It will treat dates and currency as ordinary numbers.

Probably, each respondent belongs to one “village,” each village to one region and each village has one population figure. But the app does not need to know that.

In each case, the app also adds a “statement ID” field, numbering the statements 1, 2, 3 etc. to help keep track of them.

However, it is better to import additional data about your sources (and questions) using separate tables.

### Uploading/importing Source (Respondent) and Question data separately

Often it isn’t convenient to provide this whole table yourself. Often you think in terms of smaller sub-tables. For example you might have one table just with the statements and the respondents (say, there are 20 statements per respondent and 10 respondents, so a total of 200 statements), and another table with 20 rows giving information about each respondent, say with just the field “Village” or maybe with more fields. The app can help you combine this data with the statements. Here is an example.

Suppose you already uploaded just this table

[statements](https://www.notion.so/ff4a4760f7c244c6a3cedfbf227f278a)

The app already added an additional statement_id field for you.

[statements](https://www.notion.so/a702ad5c767a4c499d64106604169053)

(Or, suppose you already imported QuIP data as described elsewhere.)

Suppose you now have additional information about the **sources**, like this.

[sources](https://www.notion.so/8cb2dbd7f46640cd926fffb96d171c46)

Then you can upload this table as a separate tab. The first field is called the key field and its name corresponds exactly to the name of one of the existing fields, in this case, source_id. All of the values in that field have to correspond to existing values of the key field. The name of the first column has to be exactly “source_id” (or “question_id” if you are in the Questions table, uploading questions).

Upload this data by clicking “Sources” in the data editor on the right (where you will see any existing data for your sources) and then selecting your csv file.

If you are uploading additional data about *Sources* remember to click the Sources button first. If you are uploading additional data about *Questions* remember to click the Questions button first.

The app merges the information like this:

[statements](https://www.notion.so/69e73c60edfa423489808c67c3802255)

The procedure is exactly analogous for uploading additional information about your questions, for example question text, or questionnaire section.

You can upload new additional data at any time. Plus, if you want to *change* existing source or question data, you just upload the appropriate new table which overwrites the previous data, e.g. if you find that some of your sources data is incorrect, you can either edit it manually in the table or you can upload a new version of the incorrect table and the old values will be overwritten and any new values, including values in new fields, will be added.

### Sources

### Questions

### Special case: Closed question blocks

If you give the app a single set of closed responses like better, worse then it can make tables like [this]

And in particular it will recode a fixed set of words into + positive, - negative and 0 neutral

this approach only makes sense if you have one larger set of questions with the same set of fixed reply options

You supply this data in the form of user-defined fields in the Sources table, whose names contain a *. 

This QuIP-oriented special feature is only provided for one single block of questions which each have the same set of answers, or could be recoded to have the same set of answers, e.g. twenty questions which are all answered or could be recoded as "always/sometimes/rarely/never" or whatever.

You could still, if you want, import a whole bunch of different closed questions with different answer categories as custom fields in the Sources table. So just as you can have gender as a field, you could have "answer to closed question about how often do you go hungry (never/sometimes/often)" as a field; the point of that would be so that you can e.g. do analyses like show me only the maps of the women, or the people who often go hungry, or the women who often go hungry. The point of the so-called closed questions feature is just to organise and provide the answers to a whole block of questions together in one place.

#### QuIP recodes for closed questions - live link

Now with Spanish!

[https://www.dropbox.com/s/rn5yl3kmjnkrgmd/quip-recodes.csv?dl=0](https://www.dropbox.com/s/rn5yl3kmjnkrgmd/quip-recodes.csv?dl=0) recodes = the live link


Closed question data, common in QuIP studies, are additional data fields which contain closed responses like these:

- better
- get better
- improved
- a bad change
- worse
- no change
- stayed the same

etc.

The current list of recodes is [here](https://www.dropbox.com/s/j8mn0i7w2ahtw56/quip-recodes.csv?dl=0).

This kind of data is automatically recoded into +, - and 0, and displayed in the Tables panel if you select “closed questions.”

## Upload and roundtripping: examples

1. Download any current file and do any of the following:
    1. make any ordinary trivial edits like changing a name, and/or add ordinary columns/fields, upload, must just work
    2. Same, but optionally delete any of the main 5 tabs
    3. Add rows. Note this does **not** add linked entries in linked tables.
    4. Delete rows. Note this does **not** delete linked entries in linked tables.
2. Only with a statements tab, 
    1. with a column text and optionally source_id, question_id and maybe other fields. should this add the source and question ids to the source and question tables? 
3. Only with a links tab 
    1. which has just from_label and to_label columns (and no from or to columns but optionally others) and no factors tab 
    2. which has numerical from and to columns  which (optionally, usually) relate at least partially to a factors tab which has a factor_id column and (optionally, usually) a label column.
    3. also with statement ids? should it create these in the statements table? 
4. Just a sources and/or questions tab
    - should be straightforward?

## Preparing and uploading/importing your .xlsx file

You provide this table, and any other tables you might need as one xlsx file. 

Your statements can have non-Latin characters like à, è, ù, б, ж, etc, the app should reproduce these perfectly.

### What if you have merged cells so e.g. source_id spans many rows?

[https://www.extendoffice.com/documents/excel/1139-excel-unmerge-cells-and-fill.html](https://www.extendoffice.com/documents/excel/1139-excel-unmerge-cells-and-fill.html)

## 💡Uploading data in a hybrid format which includes statements and additional data in the same sheet

Ask us for a clever Excel file which can restructure your data for you.

- paste the raw data into a tab in a new xlsc file
- ctrl T to make into table, table name = raw
- data / get data / from table, range
- rename columns source_id etc
- close & load
- rightclick on raw / reference
- rename as statements
- filter contains $
- rightclick on raw / reference
- query / reference raw
- rename as questions
- delete columns except question_id and question_text
- remove rows / remove duplicates
- rightclick on raw / reference
- rename as sources
- filter: = Table.SelectRows(#"Added Custom", each not Text.StartsWith([question_id], "$") or Text.Contains([question_id], "*") or Text.Contains([question_id], "#"))
- add column / custom column
- q= [question_id]&"-"&[question_text]
- remove those two columns
- click transform / pivot column: pivot on q, text as values, don't aggregate
- home/ close reload
- close and copy the file, save the copy as xls

## Importing from other software

We are aware of the initiative to make qualitative coding software interoperable, [https://www.qdasoftware.org/products-project-exchange/](https://www.qdasoftware.org/products-project-exchange/). Causal Map has quite a different model and at the moment we don’t provide this, but would do if there is a lot of interest.

You can always export all your data at any time using the buttons in the File tab.

### Exporting for import into kumu.io

tbc

### Exporting for import into NodeXL

tbc

### Importing existing causal coding

You might already have causal information, for example in the form of node and edge lists, which you want to import. This is not currently possible using the UI: please get in touch.

### QuIP-specific: Importing from a QuIP spreadsheet

This section is specifically about importing from a QuIP-style spreadsheet. This format has many rows, one for each answer, and includes the statements as one kind of answer; you can import statements and additional data from the same fieldwork file. The main differences to a normal import are:

- Rows whose question code does not include a “$” will be imported only as additional data, not as statements.
- A new question ID will be constructed from the question code together with the question text.

First save the main data sheet as a .csv, then press button X to import it. Here you can upload all your statements at the start of a file.

Your csv file must have just four columns, with column headers in this order:

- `text`, containing the answers
- `#SourceID`, containing the respondent ID
- `#QuestionID`, containing the respondent ID,
- `Question`, containing the question text

(Actually the names of the columns does not matter, only the order.)

Your Question IDs may include the following characters:

A dollar `$` to be treated as a statement

A star `*` to be treated as a closed question.

A hash `#` to be treated as important additional data.

Otherwise your question will be treated as unimportant additional data.

You can mix these, so e.g.

$*# Do you have a house

Will be treated as a statement and as closed question (part of a block) and as important additional data.